{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_QoFgQ22qwO1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import regex as re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def file_to_sentence_list(file_path):\n",
        "\twith open(file_path, 'r') as file:\n",
        "\t\ttext = file.read()\n",
        "\n",
        "\t# Splitting the text into sentences using\n",
        "\t# delimiters like '.', '?', and '!'\n",
        "\tsentences = [sentence.strip() for sentence in re.split(\n",
        "\t\tr'(?<=[.!?])\\s+', text) if sentence.strip()]\n",
        "\n",
        "\treturn sentences\n",
        "\n",
        "file_path = 'pizza.txt'\n",
        "text_data = file_to_sentence_list(file_path)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_data)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create input sequences\n",
        "input_sequences = []\n",
        "for line in text_data:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences and split into predictors and label\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(\n",
        "\tinput_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "# Convert target data to one-hot encoding\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n"
      ],
      "metadata": {
        "id": "GG3AwqP2s3rF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 10,\n",
        "\t\t\t\t\tinput_length=max_sequence_len-1))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "\t\t\toptimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "JAV5YGbGukuf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X, y, epochs=100, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjocOUw4uoZY",
        "outputId": "75b0853b-98a0-4d57-dbee-4afad189ad53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "51/51 [==============================] - 6s 47ms/step - loss: 6.2298 - accuracy: 0.0405\n",
            "Epoch 2/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 5.7899 - accuracy: 0.0565\n",
            "Epoch 3/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 5.7283 - accuracy: 0.0565\n",
            "Epoch 4/100\n",
            "51/51 [==============================] - 4s 77ms/step - loss: 5.7040 - accuracy: 0.0534\n",
            "Epoch 5/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 5.6706 - accuracy: 0.0565\n",
            "Epoch 6/100\n",
            "51/51 [==============================] - 2s 46ms/step - loss: 5.6219 - accuracy: 0.0534\n",
            "Epoch 7/100\n",
            "51/51 [==============================] - 2s 46ms/step - loss: 5.5718 - accuracy: 0.0584\n",
            "Epoch 8/100\n",
            "51/51 [==============================] - 3s 49ms/step - loss: 5.5177 - accuracy: 0.0639\n",
            "Epoch 9/100\n",
            "51/51 [==============================] - 4s 78ms/step - loss: 5.4725 - accuracy: 0.0602\n",
            "Epoch 10/100\n",
            "51/51 [==============================] - 3s 53ms/step - loss: 5.4239 - accuracy: 0.0719\n",
            "Epoch 11/100\n",
            "51/51 [==============================] - 3s 49ms/step - loss: 5.3644 - accuracy: 0.0768\n",
            "Epoch 12/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 5.2971 - accuracy: 0.0805\n",
            "Epoch 13/100\n",
            "51/51 [==============================] - 3s 65ms/step - loss: 5.2211 - accuracy: 0.0792\n",
            "Epoch 14/100\n",
            "51/51 [==============================] - 3s 58ms/step - loss: 5.1356 - accuracy: 0.0762\n",
            "Epoch 15/100\n",
            "51/51 [==============================] - 3s 53ms/step - loss: 5.0388 - accuracy: 0.0805\n",
            "Epoch 16/100\n",
            "51/51 [==============================] - 3s 53ms/step - loss: 4.9400 - accuracy: 0.0878\n",
            "Epoch 17/100\n",
            "51/51 [==============================] - 3s 50ms/step - loss: 4.8479 - accuracy: 0.0958\n",
            "Epoch 18/100\n",
            "51/51 [==============================] - 4s 75ms/step - loss: 4.7525 - accuracy: 0.1063\n",
            "Epoch 19/100\n",
            "51/51 [==============================] - 2s 48ms/step - loss: 4.6642 - accuracy: 0.1087\n",
            "Epoch 20/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 4.5837 - accuracy: 0.1186\n",
            "Epoch 21/100\n",
            "51/51 [==============================] - 3s 50ms/step - loss: 4.4973 - accuracy: 0.1179\n",
            "Epoch 22/100\n",
            "51/51 [==============================] - 3s 60ms/step - loss: 4.4142 - accuracy: 0.1198\n",
            "Epoch 23/100\n",
            "51/51 [==============================] - 3s 60ms/step - loss: 4.3294 - accuracy: 0.1253\n",
            "Epoch 24/100\n",
            "51/51 [==============================] - 2s 48ms/step - loss: 4.2485 - accuracy: 0.1413\n",
            "Epoch 25/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 4.1758 - accuracy: 0.1431\n",
            "Epoch 26/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 4.0969 - accuracy: 0.1425\n",
            "Epoch 27/100\n",
            "51/51 [==============================] - 4s 71ms/step - loss: 4.0247 - accuracy: 0.1542\n",
            "Epoch 28/100\n",
            "51/51 [==============================] - 3s 52ms/step - loss: 3.9454 - accuracy: 0.1591\n",
            "Epoch 29/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 3.8696 - accuracy: 0.1652\n",
            "Epoch 30/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 3.7935 - accuracy: 0.1726\n",
            "Epoch 31/100\n",
            "51/51 [==============================] - 2s 48ms/step - loss: 3.7181 - accuracy: 0.1818\n",
            "Epoch 32/100\n",
            "51/51 [==============================] - 4s 74ms/step - loss: 3.6428 - accuracy: 0.1806\n",
            "Epoch 33/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 3.5683 - accuracy: 0.1892\n",
            "Epoch 34/100\n",
            "51/51 [==============================] - 3s 54ms/step - loss: 3.4937 - accuracy: 0.2027\n",
            "Epoch 35/100\n",
            "51/51 [==============================] - 3s 55ms/step - loss: 3.4226 - accuracy: 0.2162\n",
            "Epoch 36/100\n",
            "51/51 [==============================] - 3s 67ms/step - loss: 3.3482 - accuracy: 0.2236\n",
            "Epoch 37/100\n",
            "51/51 [==============================] - 3s 62ms/step - loss: 3.2744 - accuracy: 0.2359\n",
            "Epoch 38/100\n",
            "51/51 [==============================] - 3s 51ms/step - loss: 3.2049 - accuracy: 0.2512\n",
            "Epoch 39/100\n",
            "51/51 [==============================] - 3s 52ms/step - loss: 3.1347 - accuracy: 0.2727\n",
            "Epoch 40/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 3.0605 - accuracy: 0.2955\n",
            "Epoch 41/100\n",
            "51/51 [==============================] - 4s 74ms/step - loss: 2.9888 - accuracy: 0.3145\n",
            "Epoch 42/100\n",
            "51/51 [==============================] - 2s 46ms/step - loss: 2.9229 - accuracy: 0.3348\n",
            "Epoch 43/100\n",
            "51/51 [==============================] - 3s 50ms/step - loss: 2.8523 - accuracy: 0.3440\n",
            "Epoch 44/100\n",
            "51/51 [==============================] - 2s 46ms/step - loss: 2.7852 - accuracy: 0.3741\n",
            "Epoch 45/100\n",
            "51/51 [==============================] - 3s 56ms/step - loss: 2.7173 - accuracy: 0.4017\n",
            "Epoch 46/100\n",
            "51/51 [==============================] - 4s 69ms/step - loss: 2.6511 - accuracy: 0.4177\n",
            "Epoch 47/100\n",
            "51/51 [==============================] - 2s 49ms/step - loss: 2.5861 - accuracy: 0.4312\n",
            "Epoch 48/100\n",
            "51/51 [==============================] - 2s 48ms/step - loss: 2.5232 - accuracy: 0.4521\n",
            "Epoch 49/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 2.4625 - accuracy: 0.4717\n",
            "Epoch 50/100\n",
            "51/51 [==============================] - 4s 74ms/step - loss: 2.3963 - accuracy: 0.4920\n",
            "Epoch 51/100\n",
            "51/51 [==============================] - 3s 53ms/step - loss: 2.3366 - accuracy: 0.5117\n",
            "Epoch 52/100\n",
            "51/51 [==============================] - 2s 48ms/step - loss: 2.2767 - accuracy: 0.5307\n",
            "Epoch 53/100\n",
            "51/51 [==============================] - 3s 52ms/step - loss: 2.2217 - accuracy: 0.5473\n",
            "Epoch 54/100\n",
            "51/51 [==============================] - 3s 51ms/step - loss: 2.1608 - accuracy: 0.5614\n",
            "Epoch 55/100\n",
            "51/51 [==============================] - 4s 73ms/step - loss: 2.1019 - accuracy: 0.5756\n",
            "Epoch 56/100\n",
            "51/51 [==============================] - 3s 53ms/step - loss: 2.0520 - accuracy: 0.5891\n",
            "Epoch 57/100\n",
            "51/51 [==============================] - 3s 49ms/step - loss: 1.9971 - accuracy: 0.6020\n",
            "Epoch 58/100\n",
            "51/51 [==============================] - 2s 48ms/step - loss: 1.9415 - accuracy: 0.6247\n",
            "Epoch 59/100\n",
            "51/51 [==============================] - 4s 79ms/step - loss: 1.8913 - accuracy: 0.6357\n",
            "Epoch 60/100\n",
            "51/51 [==============================] - 3s 51ms/step - loss: 1.8469 - accuracy: 0.6388\n",
            "Epoch 61/100\n",
            "51/51 [==============================] - 3s 51ms/step - loss: 1.7921 - accuracy: 0.6572\n",
            "Epoch 62/100\n",
            "51/51 [==============================] - 2s 49ms/step - loss: 1.7449 - accuracy: 0.6628\n",
            "Epoch 63/100\n",
            "51/51 [==============================] - 3s 57ms/step - loss: 1.6972 - accuracy: 0.6769\n",
            "Epoch 64/100\n",
            "51/51 [==============================] - 4s 74ms/step - loss: 1.6536 - accuracy: 0.7009\n",
            "Epoch 65/100\n",
            "51/51 [==============================] - 3s 54ms/step - loss: 1.6067 - accuracy: 0.6966\n",
            "Epoch 66/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 1.5646 - accuracy: 0.7119\n",
            "Epoch 67/100\n",
            "51/51 [==============================] - 2s 48ms/step - loss: 1.5233 - accuracy: 0.7248\n",
            "Epoch 68/100\n",
            "51/51 [==============================] - 4s 76ms/step - loss: 1.4829 - accuracy: 0.7328\n",
            "Epoch 69/100\n",
            "51/51 [==============================] - 3s 51ms/step - loss: 1.4436 - accuracy: 0.7482\n",
            "Epoch 70/100\n",
            "51/51 [==============================] - 2s 48ms/step - loss: 1.4057 - accuracy: 0.7537\n",
            "Epoch 71/100\n",
            "51/51 [==============================] - 2s 48ms/step - loss: 1.3660 - accuracy: 0.7611\n",
            "Epoch 72/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 1.3308 - accuracy: 0.7666\n",
            "Epoch 73/100\n",
            "51/51 [==============================] - 4s 74ms/step - loss: 1.2960 - accuracy: 0.7819\n",
            "Epoch 74/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 1.2561 - accuracy: 0.7936\n",
            "Epoch 75/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 1.2211 - accuracy: 0.8016\n",
            "Epoch 76/100\n",
            "51/51 [==============================] - 2s 48ms/step - loss: 1.1913 - accuracy: 0.8034\n",
            "Epoch 77/100\n",
            "51/51 [==============================] - 3s 60ms/step - loss: 1.1562 - accuracy: 0.8170\n",
            "Epoch 78/100\n",
            "51/51 [==============================] - 3s 65ms/step - loss: 1.1269 - accuracy: 0.8108\n",
            "Epoch 79/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 1.0968 - accuracy: 0.8213\n",
            "Epoch 80/100\n",
            "51/51 [==============================] - 3s 51ms/step - loss: 1.0660 - accuracy: 0.8262\n",
            "Epoch 81/100\n",
            "51/51 [==============================] - 3s 55ms/step - loss: 1.0365 - accuracy: 0.8397\n",
            "Epoch 82/100\n",
            "51/51 [==============================] - 4s 74ms/step - loss: 1.0102 - accuracy: 0.8415\n",
            "Epoch 83/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 0.9818 - accuracy: 0.8495\n",
            "Epoch 84/100\n",
            "51/51 [==============================] - 3s 52ms/step - loss: 0.9589 - accuracy: 0.8569\n",
            "Epoch 85/100\n",
            "51/51 [==============================] - 3s 50ms/step - loss: 0.9292 - accuracy: 0.8587\n",
            "Epoch 86/100\n",
            "51/51 [==============================] - 3s 53ms/step - loss: 0.9097 - accuracy: 0.8649\n",
            "Epoch 87/100\n",
            "51/51 [==============================] - 4s 69ms/step - loss: 0.8846 - accuracy: 0.8661\n",
            "Epoch 88/100\n",
            "51/51 [==============================] - 3s 50ms/step - loss: 0.8584 - accuracy: 0.8716\n",
            "Epoch 89/100\n",
            "51/51 [==============================] - 3s 50ms/step - loss: 0.8369 - accuracy: 0.8747\n",
            "Epoch 90/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 0.8280 - accuracy: 0.8827\n",
            "Epoch 91/100\n",
            "51/51 [==============================] - 4s 78ms/step - loss: 0.8026 - accuracy: 0.8784\n",
            "Epoch 92/100\n",
            "51/51 [==============================] - 2s 47ms/step - loss: 0.7750 - accuracy: 0.8888\n",
            "Epoch 93/100\n",
            "51/51 [==============================] - 3s 49ms/step - loss: 0.7541 - accuracy: 0.8943\n",
            "Epoch 94/100\n",
            "51/51 [==============================] - 3s 54ms/step - loss: 0.7357 - accuracy: 0.8913\n",
            "Epoch 95/100\n",
            "51/51 [==============================] - 3s 62ms/step - loss: 0.7154 - accuracy: 0.8968\n",
            "Epoch 96/100\n",
            "51/51 [==============================] - 4s 71ms/step - loss: 0.6992 - accuracy: 0.9054\n",
            "Epoch 97/100\n",
            "51/51 [==============================] - 3s 52ms/step - loss: 0.6815 - accuracy: 0.8980\n",
            "Epoch 98/100\n",
            "51/51 [==============================] - 3s 54ms/step - loss: 0.6654 - accuracy: 0.9097\n",
            "Epoch 99/100\n",
            "51/51 [==============================] - 3s 56ms/step - loss: 0.6490 - accuracy: 0.9091\n",
            "Epoch 100/100\n",
            "51/51 [==============================] - 4s 77ms/step - loss: 0.6338 - accuracy: 0.9140\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d25e4619000>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate next word predictions\n",
        "seed_text = \"Pizza have different \"\n",
        "next_words = 5\n",
        "\n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences(\n",
        "\t\t[token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted_probs = model.predict(token_list)\n",
        "\tpredicted_word = tokenizer.index_word[np.argmax(predicted_probs)]\n",
        "\tseed_text += \" \" + predicted_word\n",
        "\n",
        "print(\"Next predicted words:\", seed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LObqZdvowkuL",
        "outputId": "bf7047c0-2a15-45d0-ee41-368d4628b261"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 496ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Next predicted words: Pizza have different  become a symbol of comfort\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate next word predictions\n",
        "seed_text = \"Beyond its traditional \"\n",
        "next_words = 5\n",
        "\n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences(\n",
        "\t\t[token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted_probs = model.predict(token_list)\n",
        "\tpredicted_word = tokenizer.index_word[np.argmax(predicted_probs)]\n",
        "\tseed_text += \" \" + predicted_word\n",
        "\n",
        "print(\"Next predicted words:\", seed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6XHb3mixA-l",
        "outputId": "07ba37b9-c1c2-4bc6-8e5c-5edd8cc7d46e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Next predicted words: Beyond its traditional  forms pizza has embraced the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Technology will play\"\n",
        "next_words = 5\n",
        "\n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences(\n",
        "\t\t[token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted_probs = model.predict(token_list)\n",
        "\tpredicted_word = tokenizer.index_word[np.argmax(predicted_probs)]\n",
        "\tseed_text += \" \" + predicted_word\n",
        "\n",
        "print(\"Next predicted words:\", seed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLM6rBMS_3Ep",
        "outputId": "a40c0017-8f89-43b5-acf9-7368b77fd76c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Next predicted words: Technology will play a significant role in shaping\n"
          ]
        }
      ]
    }
  ]
}